{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task- 3: Multilabel Classification of Counter Speech\n",
    "### This notebook is used to measure performance of the combination of different classifier and different feature engineering techniques  used in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from commen_preprocess import *\n",
    "from skmultilearn.adapt import brknn\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.problem_transform import br\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.neurofuzzy import MLARAM\n",
    "import scipy\n",
    "import sklearn.metrics\n",
    "import argparse\n",
    "import numpy\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble  import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn import neural_network\n",
    "from sklearn import linear_model\n",
    "import gensim, sklearn\n",
    "from collections import defaultdict\n",
    "#from batch_gen import batch_gen\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import os\n",
    "from string import punctuation\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import confusion_matrix,make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "ps = PorterStemmer()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.sparse import vstack, hstack\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word_to_vec model  loading \n",
    "1. change the path of glove model file\n",
    "2. One the given function must load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Loading Glove Model\n",
      "count1.0\n",
      "count2.0\n",
      "count3.0\n",
      "count4.0\n",
      "count5.0\n",
      "count6.0\n",
      "count7.0\n",
      "count8.0\n",
      "count9.0\n",
      "count10.0\n",
      "count11.0\n",
      "count12.0\n",
      "count13.0\n",
      "count14.0\n",
      "count15.0\n",
      "count16.0\n",
      "count17.0\n",
      "count18.0\n",
      "count19.0\n",
      "count20.0\n",
      "count21.0\n",
      "count22.0\n",
      "count23.0\n",
      "count24.0\n",
      "count25.0\n",
      "count26.0\n",
      "count27.0\n",
      "count28.0\n",
      "count29.0\n",
      "count30.0\n",
      "count31.0\n",
      "count32.0\n",
      "count33.0\n",
      "count34.0\n",
      "count35.0\n",
      "count36.0\n",
      "count37.0\n",
      "count38.0\n",
      "count39.0\n",
      "count40.0\n",
      "count41.0\n",
      "count42.0\n",
      "count43.0\n",
      "count44.0\n",
      "count45.0\n",
      "count46.0\n",
      "count47.0\n",
      "count48.0\n",
      "count49.0\n",
      "count50.0\n",
      "count51.0\n",
      "count52.0\n",
      "count53.0\n",
      "count54.0\n",
      "count55.0\n",
      "count56.0\n",
      "count57.0\n",
      "count58.0\n",
      "count59.0\n",
      "count60.0\n",
      "count61.0\n",
      "count62.0\n",
      "count63.0\n",
      "count64.0\n",
      "count65.0\n",
      "count66.0\n",
      "count67.0\n",
      "count68.0\n",
      "count69.0\n",
      "count70.0\n",
      "count71.0\n",
      "count72.0\n",
      "count73.0\n",
      "count74.0\n",
      "count75.0\n",
      "count76.0\n",
      "count77.0\n",
      "count78.0\n",
      "count79.0\n",
      "count80.0\n",
      "count81.0\n",
      "count82.0\n",
      "count83.0\n",
      "count84.0\n",
      "count85.0\n",
      "count86.0\n",
      "count87.0\n",
      "count88.0\n",
      "count89.0\n",
      "count90.0\n",
      "count91.0\n",
      "count92.0\n",
      "count93.0\n",
      "count94.0\n",
      "count95.0\n",
      "count96.0\n",
      "count97.0\n",
      "count98.0\n",
      "count99.0\n",
      "count100.0\n",
      "count101.0\n",
      "count102.0\n",
      "count103.0\n",
      "count104.0\n",
      "count105.0\n",
      "count106.0\n",
      "count107.0\n",
      "count108.0\n",
      "count109.0\n",
      "count110.0\n",
      "count111.0\n",
      "count112.0\n",
      "count113.0\n",
      "count114.0\n",
      "count115.0\n",
      "count116.0\n",
      "count117.0\n",
      "count118.0\n",
      "count119.0\n",
      "count120.0\n",
      "count121.0\n",
      "count122.0\n",
      "count123.0\n",
      "count124.0\n",
      "count125.0\n",
      "count126.0\n",
      "count127.0\n",
      "count128.0\n",
      "count129.0\n",
      "count130.0\n",
      "count131.0\n",
      "count132.0\n",
      "count133.0\n",
      "count134.0\n",
      "count135.0\n",
      "count136.0\n",
      "count137.0\n",
      "count138.0\n",
      "count139.0\n",
      "count140.0\n",
      "count141.0\n",
      "count142.0\n",
      "count143.0\n",
      "count144.0\n",
      "count145.0\n",
      "count146.0\n",
      "count147.0\n",
      "count148.0\n",
      "count149.0\n",
      "count150.0\n",
      "count151.0\n",
      "count152.0\n",
      "count153.0\n",
      "count154.0\n",
      "count155.0\n",
      "count156.0\n",
      "count157.0\n",
      "count158.0\n",
      "count159.0\n",
      "count160.0\n",
      "count161.0\n",
      "count162.0\n",
      "count163.0\n",
      "count164.0\n",
      "count165.0\n",
      "count166.0\n",
      "count167.0\n",
      "count168.0\n",
      "count169.0\n",
      "count170.0\n",
      "count171.0\n",
      "count172.0\n",
      "count173.0\n",
      "count174.0\n",
      "count175.0\n",
      "count176.0\n",
      "count177.0\n",
      "count178.0\n",
      "count179.0\n",
      "count180.0\n",
      "count181.0\n",
      "count182.0\n",
      "count183.0\n",
      "count184.0\n",
      "count185.0\n",
      "count186.0\n",
      "count187.0\n",
      "count188.0\n",
      "count189.0\n",
      "count190.0\n",
      "count191.0\n",
      "count192.0\n",
      "count193.0\n",
      "count194.0\n",
      "count195.0\n",
      "count196.0\n",
      "count197.0\n",
      "count198.0\n",
      "count199.0\n",
      "count200.0\n",
      "count201.0\n",
      "count202.0\n",
      "count203.0\n",
      "count204.0\n",
      "count205.0\n",
      "count206.0\n",
      "count207.0\n",
      "count208.0\n",
      "count209.0\n",
      "count210.0\n",
      "count211.0\n",
      "count212.0\n",
      "count213.0\n",
      "count214.0\n",
      "count215.0\n",
      "count216.0\n",
      "count217.0\n",
      "count218.0\n",
      "count219.0\n",
      "Done. 2196016  words loaded!\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import os\n",
    "#GLOVE_MODEL_FILE_200 = \"../../../LEAM-master/glove.twitter.27B/glove.twitter.27B.200d.txt\"\n",
    "GLOVE_MODEL_FILE=\"../../glove.840B.300d.txt\"\n",
    "print(os.path.isfile(GLOVE_MODEL_FILE))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## change the embedding dimension according to the model\n",
    "EMBEDDING_DIM = 300\n",
    "###change the method type\n",
    "select_method=1\n",
    "\n",
    "### method one\n",
    "def loadGloveModel1(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    i=0\n",
    "    for line in f:\n",
    "        i=i+1\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        model[word] = embedding\n",
    "        if(i%10000==0):\n",
    "            print(\"count\"+str(i/10000))\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "### method two\n",
    "def loadGloveModel2(glove_file):\n",
    "    tmp_file = get_tmpfile(\"test_crawl_200.txt\")\n",
    "\n",
    "    # call glove2word2vec script\n",
    "    # default way (through CLI): python -m gensim.scripts.glove2word2vec --input <glove_file> --output <w2v_file>\n",
    "\n",
    "    glove2word2vec(glove_file, tmp_file)\n",
    "    model=KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    return model\n",
    "\n",
    "\n",
    "if (select_method==1):\n",
    "    word2vec_model = loadGloveModel1(GLOVE_MODEL_FILE)\n",
    "elif (select_method==2):\n",
    "    word2vec_model = loadGloveModel2(GLOVE_MODEL_FILE)\n",
    "else:\n",
    "    print(\"wrong method number selected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function is generating the classification report\n",
    "1. input: ground_truth and predicted outputs\n",
    "2. output: dataframe containing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##saving the classification report\n",
    "def pandas_classification_report(y_true, y_pred):\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='macro'))\n",
    "    avg.append(accuracy_score(y_true, y_pred, normalize=True))\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support','accuracy']\n",
    "    list_all=list(metrics_summary)\n",
    "    list_all.append(cm.diagonal())\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list_all,\n",
    "        index=metrics_sum_index)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    avg[-2] = total\n",
    "\n",
    "    class_report_df['avg / total'] = avg\n",
    "\n",
    "    return class_report_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Universal Sentence Encoder configuration\n",
    "###### prerequisite: tensorflow version >=1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_0:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_0\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_1:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_1\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_10:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_10\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_11:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_11\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_12:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_12\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_13:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_13\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_14:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_14\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_15:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_15\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_16:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_16\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_2:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_2\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_3:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_3\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_4:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_4\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_5:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_5\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_6:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_6\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_7:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_7\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_8:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_8\n",
      "INFO:tensorflow:Initialize variable module_1/Embeddings_en/sharded_9:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_9\n",
      "INFO:tensorflow:Initialize variable module_1/Encoder_en/DNN/ResidualHidden_0/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Encoder_en/DNN/ResidualHidden_0/weights\n",
      "INFO:tensorflow:Initialize variable module_1/Encoder_en/DNN/ResidualHidden_1/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Encoder_en/DNN/ResidualHidden_1/weights\n",
      "INFO:tensorflow:Initialize variable module_1/Encoder_en/DNN/ResidualHidden_2/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Encoder_en/DNN/ResidualHidden_2/weights\n",
      "INFO:tensorflow:Initialize variable module_1/Encoder_en/DNN/ResidualHidden_3/projection:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Encoder_en/DNN/ResidualHidden_3/projection\n",
      "INFO:tensorflow:Initialize variable module_1/Encoder_en/DNN/ResidualHidden_3/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Encoder_en/DNN/ResidualHidden_3/weights\n",
      "INFO:tensorflow:Initialize variable module_1/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias\n",
      "INFO:tensorflow:Initialize variable module_1/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights\n",
      "INFO:tensorflow:Initialize variable module_1/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias\n",
      "INFO:tensorflow:Initialize variable module_1/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights\n",
      "INFO:tensorflow:Initialize variable module_1/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias\n",
      "INFO:tensorflow:Initialize variable module_1/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights\n",
      "INFO:tensorflow:Initialize variable module_1/SNLI/Classifier/LinearLayer/bias:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SNLI/Classifier/LinearLayer/bias\n",
      "INFO:tensorflow:Initialize variable module_1/SNLI/Classifier/LinearLayer/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SNLI/Classifier/LinearLayer/weights\n",
      "INFO:tensorflow:Initialize variable module_1/SNLI/Classifier/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SNLI/Classifier/tanh_layer_0/bias\n",
      "INFO:tensorflow:Initialize variable module_1/SNLI/Classifier/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SNLI/Classifier/tanh_layer_0/weights\n",
      "INFO:tensorflow:Initialize variable module_1/global_step:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with global_step\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "embed = hub.Module(module_url)\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=12,\n",
    "                       allow_soft_placement=True, device_count = {'CPU': 12})\n",
    "\n",
    "def get_embeddings(messages):\n",
    "      \n",
    "    with tf.Session(config=config) as session:\n",
    "            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "            message_emb = session.run(embed(messages))\n",
    "            \n",
    "    print(\"ending\")\n",
    "    return np.array(message_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset is loaded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### set the path of the the json file where to load the data \n",
    "path='../Counter_NonCounter/Data/Counterspeech_Dataset.json'\n",
    "with open(path) as fp:\n",
    "    train_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### return label\n",
    "def convert_class_label(input_text):\n",
    "    if input_text:\n",
    "        return 'counter'\n",
    "    else:\n",
    "        return 'noncounter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data in the dataframe  having the four fields as\n",
    "1. id\n",
    "2. class\n",
    "3. community\n",
    "4. category(labels)\n",
    "5. text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Loading Completed...\n"
     ]
    }
   ],
   "source": [
    "pd_train = pd.DataFrame(columns=['id','class','community','category','text'])\n",
    "\n",
    "for count, each in enumerate(train_data):\n",
    "    try:\n",
    "        pd_train.loc[count]  = [each['id'], convert_class_label(each['CounterSpeech']), each['Community'],each['Category'],each['commentText']]\n",
    "    except:\n",
    "        pass\n",
    "print('Training Data Loading Completed...') \n",
    "\n",
    "### select the data having the labels ...default denotes the non counter speech\n",
    "pd_train =pd_train[pd_train['category']!='Default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of the category column into a multi-hot vector encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list1=[[],[],[],[],[],[],[],[],[],[]]\n",
    "for ele in pd_train['category']:\n",
    "    temp=[]\n",
    "    #print(ele)\n",
    "    for i in range(0,len(ele),2):\n",
    "        temp.append(ord(ele[i])-ord('0'))\n",
    "    #print(temp)\n",
    "    if(len(temp)==0):\n",
    "        print(temp)\n",
    "    for i in range(0,10):\n",
    "        if i+1 in temp:\n",
    "            list1[i].append(1)\n",
    "        else:\n",
    "            list1[i].append(0)\n",
    "y_train=np.array([np.array(xi) for xi in list1])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### final dataframe for the task created \n",
    "pd_train = pd.DataFrame({'text':list(pd_train['text']),'cat0':list1[0],'cat1':list1[1],'cat2':list1[2],'cat3':list1[3],'cat4':list1[4],'cat5':list1[5],'cat6':list1[6],'cat7':list1[7],'cat8':list1[8],'cat9':list1[9]})\n",
    "### drop the entries having blank entries\n",
    "pd_train['text'].replace('', np.nan, inplace=True)\n",
    "pd_train.dropna(subset=['text'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>What kind of God is it that Hates people &amp; Cur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Question - Which comment is racist and why?\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>That Israel fail that is nothing new. That Isr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>This god damned ignorant little prick doesn't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>our shit may be cooler but... police brutality...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat0  cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  \\\n",
       "0     0     1     0     0     0     0     0     1     0     0   \n",
       "1     0     0     0     0     0     0     0     1     0     0   \n",
       "2     0     0     0     0     0     0     0     1     0     0   \n",
       "3     0     0     0     0     0     0     0     0     1     0   \n",
       "4     0     1     0     0     0     0     0     0     0     0   \n",
       "\n",
       "                                                text  \n",
       "0  What kind of God is it that Hates people & Cur...  \n",
       "1  Question - Which comment is racist and why?\\n ...  \n",
       "2  That Israel fail that is nothing new. That Isr...  \n",
       "3  This god damned ignorant little prick doesn't ...  \n",
       "4  our shit may be cooler but... police brutality...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### converting the data into text and labels dictionary\n",
    "def get_data():\n",
    "    comments=pd_train['text'].values\n",
    "    df = pd_train.drop('text', 1)\n",
    "    labels=df.values\n",
    "    list_comment=[]\n",
    "    for comment,label in zip(comments,labels):\n",
    "        temp={}\n",
    "        temp['text']=comment\n",
    "        temp['label']=label\n",
    "        list_comment.append(temp)\n",
    "    return list_comment    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different type of tokenization that can be performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### stopwords and punctuations are not removed but text is cleaned and stemmed\n",
    "def glove_tokenize_norem(text):\n",
    "    #text = tokenizer(text)\n",
    "    text=clean(text, remove_stopwords=False, remove_punctuations=False)\n",
    "    words = text.split()\n",
    "    words =[ps.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "####stopwords and punctuations are removed along with that text is cleaned and stemmed\n",
    "def glove_tokenize(text):\n",
    "    #text = tokenizer(text)\n",
    "    text=clean(text, remove_stopwords=False, remove_punctuations=False)\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in STOPWORDS]\n",
    "    words =[ps.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "####stopwords and punctuations are removed along with that text is cleaned\n",
    "def glove_tokenize_embed(text):\n",
    "    #text = tokenizer(text)\n",
    "    text=clean(text, remove_stopwords=False, remove_punctuations=False)\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in STOPWORDS]\n",
    "    words =[ps.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different feature generation methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOKENIZER = glove_tokenize\n",
    "#google encoding used where text is cleaned  \n",
    "def gen_data_google():\n",
    "    comments = get_data()\n",
    "    X, y = [], []\n",
    "    for comment in comments:\n",
    "        y.append(comment['label'])\n",
    "        #X.append(tokenizer(comment['text']))\n",
    "        X.append(clean(comment['text'], remove_stopwords=True, remove_punctuations=True))\n",
    "    \n",
    "    X =get_embeddings(X)\n",
    "    return X, y\n",
    "\n",
    "#google encoding used where text is not cleaned \n",
    "def gen_data_google2():\n",
    "    comments = get_data()\n",
    "    X, X1, y = [],[],[]\n",
    "    for comment in comments:\n",
    "        y.append(comment['label'])\n",
    "        X.append(clean(comment['text'], remove_stopwords=False, remove_punctuations=False))\n",
    "    #Word Level Features\n",
    "    X =get_embeddings(X)\n",
    "    return X,y\n",
    "\n",
    "### tfidf feature generation was used here where stopwords and punctuations are removed \n",
    "def gen_data_new_tfidf():\n",
    "    comments = get_data()\n",
    "    X, y = [], []\n",
    "    for comment in comments:\n",
    "        y.append(comment['label'])\n",
    "        X.append(comment['text'])\n",
    "\n",
    "\n",
    "    #Word Level Features\n",
    "    word_vectorizer = TfidfVectorizer(sublinear_tf=False, ngram_range=(1,3),\n",
    "                min_df=1, \n",
    "                strip_accents='unicode',\n",
    "                #smooth_idf=1,\n",
    "                analyzer='word', \n",
    "                stop_words='english',\n",
    "                tokenizer=TOKENIZER,             \n",
    "                max_features=500)\n",
    "    \n",
    "    \n",
    "    #charlevel features new\n",
    "    char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=False,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    #stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=500)\n",
    "    word_vectorizer.fit(X)\n",
    "    char_vectorizer.fit(X)\n",
    "    test_word_features = word_vectorizer.transform(X)\n",
    "    test_char_features = char_vectorizer.transform(X)\n",
    "    X = list(hstack([test_char_features, test_word_features]).toarray())\n",
    "    return X, y\n",
    "\n",
    "### tfidf feature generation was used here where stopwords and punctuations are not removed \n",
    "def gen_data_new_tfidf2():\n",
    "    comments = get_data()\n",
    "    X, y = [], []\n",
    "    for comment in comments:\n",
    "        y.append(comment['label'])\n",
    "        X.append(comment['text'])\n",
    "\n",
    "\n",
    "    #Word Level Features\n",
    "    word_vectorizer = TfidfVectorizer(sublinear_tf=False,ngram_range=(1,3),\n",
    "                min_df=1, \n",
    "                strip_accents='unicode',\n",
    "                #smooth_idf=1,\n",
    "                analyzer='word', \n",
    "                #stop_words='english',\n",
    "                tokenizer=glove_tokenize_norem,             \n",
    "                max_features=500)\n",
    "    \n",
    "    \n",
    "    #charlevel features new\n",
    "    char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=False,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    #stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=500)\n",
    "    \n",
    "    word_vectorizer.fit(X)\n",
    "    char_vectorizer.fit(X)\n",
    "    test_word_features = word_vectorizer.transform(X)\n",
    "    test_char_features = char_vectorizer.transform(X)\n",
    "    X = list(hstack([test_char_features, test_word_features]).toarray())\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def gen_data_embed():\n",
    "    comments = get_data()\n",
    "    X, y = [], []\n",
    "    for comment in comments:\n",
    "        words = glove_tokenize_embed(comment['text'].lower())\n",
    "        emb = numpy.zeros(EMBEDDING_DIM)\n",
    "        for word in words:\n",
    "            try:\n",
    "                emb += word2vec_model[word]\n",
    "            except:\n",
    "                pass\n",
    "        emb /= len(words)\n",
    "        X.append(emb)\n",
    "        y.append(comment['label'])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "## combination of not cleaned google encodings and tfidf features where stopwords and punctuations are not removed \n",
    "def combine_tf_google_rem():\n",
    "    X,_=gen_data_google()\n",
    "    X1,y=gen_data_new_tfidf()\n",
    "    X=np.concatenate((np.array(X), np.array(X1)), axis=1)\n",
    "    return X,y\n",
    "\n",
    "## combination of cleaned google encodings and tfidf features where stopwords and punctuations are ssremoved \n",
    "def combine_tf_google_norem():\n",
    "    X,_=gen_data_google2()\n",
    "    X1,y=gen_data_new_tfidf2()\n",
    "    X=np.concatenate((np.array(X), np.array(X1)), axis=1)\n",
    "    return X,y\n",
    "## combination of google encodings where stopwords and punctuation are kept and tfidf features where stopwords and punctuations are removed \n",
    "def combine_tf_rem_google_norem():\n",
    "    X,_=gen_data_google2() \n",
    "    X1,y=gen_data_new_tfidf()\n",
    "    X=np.concatenate((np.array(X), np.array(X1)), axis=1)\n",
    "    return X,y\n",
    "## combination of google encodings where stopwords and punctuation are removed and tfidf features where stopwords and punctuations are kept \n",
    "def combine_tf_norem_google_rem():\n",
    "    X,_=gen_data_google()\n",
    "    X1,y=gen_data_new_tfidf2()\n",
    "    X=np.concatenate((np.array(X), np.array(X1)), axis=1)\n",
    "    return X,y\n",
    "\n",
    "## combination of google encodings where stopwords and punctuation are removed and average word embeddings  \n",
    "def combine_google_rem_embed():\n",
    "    X,_=gen_data_google()\n",
    "    X1,y=gen_data_embed()\n",
    "    X=np.concatenate((np.array(X), np.array(X1)), axis=1)\n",
    "    return X,y\n",
    "## combination of tfidf features where stopwords and punctuation are removed and average word embeddings  \n",
    "def combine_tf_rem_embed():\n",
    "    X,_=gen_data_new_tfidf()\n",
    "    X1,y=gen_data_embed()\n",
    "    X=np.concatenate((np.array(X), np.array(X1)), axis=1)\n",
    "    return X,y\n",
    "\n",
    "####combination of three\n",
    "def combine_tf_rem_google_norem_embed():\n",
    "    X,_=gen_data_google2()\n",
    "    X1,y=gen_data_new_tfidf()\n",
    "    X2,_=gen_data_embed()\n",
    "    X=np.concatenate((np.array(X), np.array(X1),np.array(X2)), axis=1)\n",
    "    return X,y\n",
    "\n",
    "def combine_tf_rem_google_rem_embed():\n",
    "    X,_=gen_data_google()\n",
    "    X1,y=gen_data_new_tfidf()\n",
    "    X2,_=gen_data_embed()\n",
    "    X=np.concatenate((np.array(X), np.array(X1),np.array(X2)), axis=1)\n",
    "    return X,y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###old tfidf\n",
    "\n",
    "def gen_data_old_tfidf():\n",
    "    comments = get_data()\n",
    "    X, y = [], []\n",
    "    for comment in comments:\n",
    "        y.append(comment['label'])\n",
    "        X.append(comment['text'])\n",
    "    with open('../tfidf_word_vectorizer.pk', 'rb') as fin:\n",
    "        word_vectorizer = pickle.load(fin)\n",
    "\n",
    "    with open('../tfidf_char_vectorizer.pk', 'rb') as fin:\n",
    "        char_vectorizer = pickle.load(fin)\n",
    "\n",
    "\n",
    "    \n",
    "    word_vectorizer.fit(X)\n",
    "    char_vectorizer.fit(X)\n",
    "    \n",
    "    test_word_features = word_vectorizer.transform(X)\n",
    "    test_char_features = char_vectorizer.transform(X)\n",
    "    X = list(hstack([test_char_features, test_word_features]).toarray())\n",
    "    #y = MultiLabelBinarizer(classes = (1,2,3,4,5,6,7,8,9,10)).fit_transform(y)\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###this is the metric used for calculating the scores \n",
    "def calculate_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "\n",
    "    acc_list = []\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "\n",
    "    for i in range(y_true.shape[0]):\n",
    "\n",
    "        set_true = set( numpy.where(y_true[i])[0] )\n",
    "        set_pred = set( numpy.where(y_pred[i])[0] )\n",
    "        #print(set_true)\n",
    "        #print(set_pred)\n",
    "        \n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/float( len(set_true.union(set_pred)) )\n",
    "            temp_acc = len(set_true.intersection(set_pred))/float( len(set_true.union(set_pred)) )\n",
    "            if len(set_pred) == 0:\n",
    "                temp_pre = 0\n",
    "            else:\n",
    "                temp_pre = len(set_true.intersection(set_pred))/float( len(set_pred) )\n",
    "            temp_rec = len(set_true.intersection(set_pred))/float( len(set_true))\n",
    "            temp_f1 = 2*(len(set_true.intersection(set_pred)))/float(len(set_pred) + len(set_true) )\n",
    "        #print('tmp_a*: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "        accuracy.append(temp_acc)\n",
    "        precision.append(temp_pre)\n",
    "        recall.append(temp_rec)\n",
    "        f1_score.append(temp_f1)\n",
    "        \n",
    "        \n",
    "    mean_hamming=sklearn.metrics.hamming_loss(y_true, y_pred)\n",
    "    mean_accuracy=numpy.mean(accuracy)\n",
    "    mean_precision=numpy.mean(precision)\n",
    "    mean_recall=numpy.mean(recall)\n",
    "    mean_fscore=(2*mean_precision*mean_recall)/(mean_precision+mean_recall)\n",
    "    return  mean_hamming,mean_accuracy,mean_precision,mean_recall,mean_fscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_accuracy_score(y_train,y_train_pred):\n",
    "    count=0\n",
    "    for ele1,ele2 in zip(y_train,y_train_pred):\n",
    "        if(list(ele1)==list(ele2)):\n",
    "            count=count+1\n",
    "    return count/y_train.shape[0]         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Classification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('multilabel_all_parameters.json') as f:\n",
    "        parameters=json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_model(model_name,featureset_name,model_type=None):\n",
    "    X,Y=get_feature(featureset_name)\n",
    "    model=get_model(model_type)\n",
    "    try:\n",
    "        model_parameter=parameters[classifier_model+'+'feature_model]\n",
    "        for k,v in param_set.items():\n",
    "             setattr(model_parameter,k,v)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    if(model==None):\n",
    "        return 1\n",
    "    path=model_name+'_'+featureset_name\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n",
    "    X = numpy.array(X)\n",
    "    Y = numpy.array(Y)\n",
    "    ham_list=[]\n",
    "    acc_list=[]\n",
    "    pre_list=[]\n",
    "    rec_list=[]\n",
    "    f1_list=[]\n",
    "    hard_metric_list=[]\n",
    "    hard_train_list=[]\n",
    "    y_total_preds=[] \n",
    "    y_total=[]\n",
    "    count=1\n",
    "    for train_index, test_index in mskf.split(X, Y):\n",
    "            #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            print(\"crossval step--\",str(count))\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = Y[train_index], Y[test_index]    \n",
    "            clf = model\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred =  clf.predict(X_test)\n",
    "            y_train_pred =  clf.predict(X_train)\n",
    "            \n",
    "            if(scipy.sparse.issparse(y_pred)):\n",
    "               ham,acc,pre,rec,f1=calculate_score(y_test,y_pred.toarray())\n",
    "               for ele in y_test:\n",
    "                  y_total.append(ele)\n",
    "               for ele in y_pred.toarray():\n",
    "                  y_total_preds.append(ele)\n",
    "               accuracy_train=accuracy_score(y_train,y_train_pred.toarray())\n",
    "               accuracy_test=accuracy_score(y_test,y_pred.toarray())\n",
    "            \n",
    "            else:\n",
    "               ham,acc,pre,rec,f1=calculate_score(y_test,y_pred)\n",
    "               for ele in y_test:\n",
    "                  y_total.append(ele)\n",
    "               for ele in y_pred:\n",
    "                  y_total_preds.append(ele)\n",
    "               accuracy_train=my_accuracy_score(y_train,y_train_pred)\n",
    "               accuracy_test=my_accuracy_score(y_test,y_pred)\n",
    "             \n",
    "            ham_list.append(ham)\n",
    "            acc_list.append(acc)\n",
    "            pre_list.append(pre)\n",
    "            rec_list.append(rec)\n",
    "            f1_list.append(f1)\n",
    "            hard_train_list.append(accuracy_train)\n",
    "            hard_metric_list.append(accuracy_test)\n",
    "            count=count+1\n",
    "    y_total_preds=np.array(y_total_preds) \n",
    "    y_total=np.array(y_total)\n",
    "    with open('all_preds_multilabel.pkl', 'wb') as f:\n",
    "        pickle.dump([y_total,y_total_preds], f)\n",
    "    \n",
    "    for i in range(10):\n",
    "        df_result=pandas_classification_report(y_total[:,i],y_total_preds[:,i])\n",
    "        df_result.to_csv(path+'/report'+str(i)+'.csv')\n",
    "        \n",
    "    \n",
    "    f = open(path+'/final_report.txt', \"w\")\n",
    "    f.write(model_name)    \n",
    "    f.write(\"The hard train score is :- \" + str(numpy.mean(hard_train_list)))\n",
    "    f.write(\"The hard metric score is :- \" + str(numpy.mean(hard_metric_list)))\n",
    "    f.write(\"The accuracy is :- \" + str(numpy.mean(acc_list)))\n",
    "    f.write(\"The precision is :- \" + str(numpy.mean(pre_list)))\n",
    "    f.write(\"The recall is :- \" + str(numpy.mean(rec_list)))\n",
    "    f.write(\"The f1_score is :- \" + str(numpy.mean(f1_list)))\n",
    "    f.write(\"The hamming loss is :-\" + str(numpy.mean(ham_list)))\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(m_type=None):\n",
    "    if not m_type:\n",
    "        print(\"ERROR: Please specify a model type!\")\n",
    "        return None\n",
    "    if m_type == 'decision_tree_classifier':\n",
    "        logreg = tree.DecisionTreeClassifier(class_weight='balanced')\n",
    "    elif m_type == 'MLPClassifier':\n",
    "        logreg = neural_network.MLPClassifier((500))\n",
    "    elif m_type == 'KNeighborsClassifier':\n",
    "        logreg = neighbors.KNeighborsClassifier(n_neighbors = 10)\n",
    "    elif m_type == 'ExtraTreeClassifier':\n",
    "        logreg = tree.ExtraTreeClassifier()\n",
    "    elif m_type == 'ExtraTreeClassifier_2':\n",
    "        logreg = ensemble.ExtraTreesClassifier()\n",
    "    elif m_type == 'Logistic_Regression':\n",
    "        logreg = OneVsRestClassifier(linear_model.LogisticRegression(class_weight='balanced'))\n",
    "    elif m_type == 'RandomForestClassifier':\n",
    "        logreg = ensemble.RandomForestClassifier(class_weight='balanced')\n",
    "    elif m_type == 'binary_relevance_GaussianNB':\n",
    "        logreg = OneVsRestClassifier(GaussianNB())\n",
    "    elif m_type == 'SVC':\n",
    "        logreg = OneVsRestClassifier(SVC(class_weight='balanced'))\n",
    "    elif m_type == 'XG_BOOST':\n",
    "        ###best_model####\n",
    "        logreg = OneVsRestClassifier(XGBClassifier(eval_metric='logloss',objective= 'binary:logistic', nthread=12, scale_pos_weight=5))\n",
    "    elif m_type == 'Catboost':\n",
    "        logreg = OneVsRestClassifier(CatBoostClassifier(iterations=100,silent=True,scale_pos_weight=5))\n",
    "    elif m_type == 'MLKNN':\n",
    "        logreg = MLkNN(k=8)\n",
    "    elif m_type == 'MLARAM':\n",
    "        logreg = MLARAM(threshold=0.05, vigilance=0.95)\n",
    "    else:\n",
    "        print(\"ERROR: Please specify a correct model\")\n",
    "        return None\n",
    "\n",
    "    return logreg\n",
    "\n",
    "\n",
    "def get_feature(f_type=None):\n",
    "    if not f_type:\n",
    "        print(\"ERROR: Please specify a model type!\")\n",
    "        return None,None\n",
    "    if f_type == 'google_not_preprocess':\n",
    "        X,y=gen_data_google2()\n",
    "    elif f_type == 'word_to_vec_embed':\n",
    "        X,y=gen_data_embed()\n",
    "    elif f_type == 'google_preprocess':\n",
    "        X,y=gen_data_google()\n",
    "    elif f_type == 'tfidf_not_preprocess':\n",
    "        X,y=gen_data_new_tfidf2()\n",
    "    elif f_type == 'tfidf_preprocess':\n",
    "        X,y=gen_data_new_tfidf()\n",
    "    elif f_type == 'google_preprocess_tfidf_preprocess':\n",
    "        X,y=combine_tf_google_rem()\n",
    "    elif f_type == 'google_nopreprocess_tfidf_nopreprocess':\n",
    "        X,y=combine_tf_google_norem()\n",
    "    elif f_type == 'google_preprocess_tfidf_nopreprocess':\n",
    "        X,y=combine_tf_norem_google_rem()\n",
    "    elif f_type == 'google_nopreprocess_tfidf_preprocess':\n",
    "        X,y=combine_tf_rem_google_norem()\n",
    "    elif f_type == 'google_preprocess_embed':\n",
    "        X,y=combine_google_rem_embed()\n",
    "    elif f_type == 'tfidf_preprocess_embed':\n",
    "        X,y=combine_tf_rem_embed()\n",
    "    elif f_type == 'google_preprocess_tfidf_preprocess_embed':\n",
    "        ###best features####\n",
    "        X,y=combine_tf_rem_google_rem_embed()\n",
    "    else:\n",
    "        print(\"give correct feature selection\")    \n",
    "    return X,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models available\n",
    "1. decision_tree_classifier\n",
    "2. MLPClassifier\n",
    "3. KNeighborsClassifier\n",
    "4. ExtraTreeClassifier\n",
    "5. ExtraTreeClassifier_2\n",
    "6. RandomForestClassifier\n",
    "7. SVC\n",
    "8. CatboostClassifier\n",
    "9. XGB_classifier\n",
    "10. Logistic_Regression\n",
    "11. MLKNN\n",
    "12. MLARAM\n",
    "13. Gaussian Naive Bayes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Feature Models available\n",
    "1. google_not_preprocess\n",
    "2. word_to_vec_embed\n",
    "3. google_preprocess\n",
    "4. tfidf_not_preprocess\n",
    "5. tfidf_preprocess\n",
    "6. google_preprocess_tfidf_preprocess\n",
    "7. google_nopreprocess_tfidf_nopreprocess\n",
    "8. google_preprocess_tfidf_nopreprocess\n",
    "9. google_nopreprocess_tfidf_preprocess\n",
    "10. google_preprocess_embed\n",
    "11. tfidf_preprocess_embed\n",
    "12. google_preprocess_tfidf_preprocess_embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the model and the feature selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending\n",
      "crossval step-- 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 5 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 9 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossval step-- 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 5 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 9 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossval step-- 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 5 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 9 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossval step-- 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 5 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 9 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossval step-- 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 5 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 9 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossval step-- 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 5 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 9 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossval step-- 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 5 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 9 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossval step-- 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 5 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 9 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossval step-- 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 5 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 9 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossval step-- 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 5 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 9 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    }
   ],
   "source": [
    "#### abbreviated model name ####\n",
    "abv_name='XG_BOOST'\n",
    "#### feature_selection method name####\n",
    "feature_select='google_preprocess_tfidf_preprocess_embed'\n",
    "#### model name ####\n",
    "model_name='XG_BOOST'\n",
    "\n",
    "### calling of the method####\n",
    "classification_model(abv_name,feature_select,model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
