{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This jupyter notebook is used to reproduce the results of the best models given in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from skmultilearn.adapt import brknn\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.problem_transform import br\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.neurofuzzy import MLARAM\n",
    "import scipy\n",
    "import sklearn.metrics\n",
    "import argparse\n",
    "import numpy\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble  import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn import neural_network\n",
    "from sklearn import linear_model\n",
    "import gensim, sklearn\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import os\n",
    "from string import punctuation\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import confusion_matrix,make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "ps = PorterStemmer()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.sparse import vstack, hstack\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function is generating the classification report\n",
    "1. input: ground_truth and predicted outputs\n",
    "2. output: dataframe containing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file used to write preserve the results of the classfier\n",
    "# confusion matrix and precision recall fscore matrix\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    \n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function is generating the classification report\n",
    "1. input: ground_truth and predicted outputs\n",
    "2. output: dataframe containing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##saving the classification report\n",
    "def pandas_classification_report(y_true, y_pred):\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='macro'))\n",
    "    avg.append(accuracy_score(y_true, y_pred, normalize=True))\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support','accuracy']\n",
    "    list_all=list(metrics_summary)\n",
    "    list_all.append(cm.diagonal())\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list_all,\n",
    "        index=metrics_sum_index)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    avg[-2] = total\n",
    "\n",
    "    class_report_df['avg / total'] = avg\n",
    "\n",
    "    return class_report_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Multilabel results production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###this is the metric used for calculating the scores \n",
    "def calculate_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "\n",
    "    acc_list = []\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "\n",
    "    for i in range(y_true.shape[0]):\n",
    "\n",
    "        set_true = set( numpy.where(y_true[i])[0] )\n",
    "        set_pred = set( numpy.where(y_pred[i])[0] )\n",
    "        #print(set_true)\n",
    "        #print(set_pred)\n",
    "        \n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/float( len(set_true.union(set_pred)) )\n",
    "            temp_acc = len(set_true.intersection(set_pred))/float( len(set_true.union(set_pred)) )\n",
    "            if len(set_pred) == 0:\n",
    "                temp_pre = 0\n",
    "            else:\n",
    "                temp_pre = len(set_true.intersection(set_pred))/float( len(set_pred) )\n",
    "            temp_rec = len(set_true.intersection(set_pred))/float( len(set_true))\n",
    "            temp_f1 = 2*(len(set_true.intersection(set_pred)))/float(len(set_pred) + len(set_true) )\n",
    "        #print('tmp_a*: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "        accuracy.append(temp_acc)\n",
    "        precision.append(temp_pre)\n",
    "        recall.append(temp_rec)\n",
    "        f1_score.append(temp_f1)\n",
    "        \n",
    "        \n",
    "    mean_hamming=sklearn.metrics.hamming_loss(y_true, y_pred)\n",
    "    mean_accuracy=numpy.mean(accuracy)\n",
    "    mean_precision=numpy.mean(precision)\n",
    "    mean_recall=numpy.mean(recall)\n",
    "    mean_fscore=(2*mean_precision*mean_recall)/(mean_precision+mean_recall)\n",
    "    return  mean_hamming,mean_accuracy,mean_precision,mean_recall,mean_fscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_accuracy_score(y_train,y_train_pred):\n",
    "    count=0\n",
    "    for ele1,ele2 in zip(y_train,y_train_pred):\n",
    "        if(list(ele1)==list(ele2)):\n",
    "            count=count+1\n",
    "    return count/y_train.shape[0]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='XGB_gptpe_multilabel'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "with open('Multilabel/all_preds_multilabel.pkl', 'rb') as f:\n",
    "        y_total,y_total_preds=pickle.load(f)\n",
    "        \n",
    "ham,acc,pre,rec,f1=calculate_score(y_total,y_total_preds)\n",
    "accuracy_test=my_accuracy_score(y_total,y_total_preds)\n",
    "                   \n",
    "for i in range(10):\n",
    "    df_result=pandas_classification_report(y_total[:,i],y_total_preds[:,i])\n",
    "    df_result.to_csv(path+'/report'+str(i)+'.csv')\n",
    "\n",
    "\n",
    "f = open(path+'/final_report.txt', \"w\")\n",
    "\n",
    "f.write(\"The hard metric score is :- \" + str(accuracy_test)+\"\\n\")\n",
    "f.write(\"The accuracy is :- \" + str(acc)+\"\\n\")\n",
    "f.write(\"The precision is :- \" + str(pre)+\"\\n\")\n",
    "f.write(\"The recall is :- \" + str(rec)+\"\\n\")\n",
    "f.write(\"The f1_score is :- \" + str(f1)+\"\\n\")\n",
    "f.write(\"The hamming loss is :-\" + str(ham)+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for binary classifier (all types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(confusion_name,report_name,pred_name_save):\n",
    "    with open(pred_name_save, 'rb') as f:\n",
    "            Classifier_Test_Y , y_pred=pickle.load(f)\n",
    "    report = classification_report(Classifier_Test_Y , y_pred )\n",
    "    cm=confusion_matrix(Classifier_Test_Y, y_pred)\n",
    "    plt=plot_confusion_matrix(cm,normalize= True,target_names = ['counter','non_counter'],title = \"Confusion Matrix\")\n",
    "    plt.savefig(confusion_name)\n",
    "    print(report)\n",
    "    df_result=pandas_classification_report(Classifier_Test_Y,y_pred)\n",
    "    df_result.to_csv(report_name,  sep=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prediction on black category trained using jew and lgbt  \n",
    "classification(confusion_name='confusion_black_CB_gptp.png',report_name='confusion_black_CB_gptp.csv',pred_name_save='Counter_NonCounter/Classifier/black_all_save.pkl')\n",
    "\n",
    "### prediction on lgbt category trained using jew and black  \n",
    "classification(confusion_name='confusion_lgbt_MLP_gp.png',report_name='confusion_lgbt_MLP_gp.csv',pred_name_save='Counter_NonCounter/Classifier/lgbt_all_save.pkl')\n",
    "\n",
    "### prediction on jew category trained using black and lgbt  \n",
    "classification(confusion_name='confusion_jew_MLP_gp.png',report_name='confusion_jew_MLP_gp.csv',pred_name_save='Counter_NonCounter/Classifier/jew_all_save.pkl')\n",
    "\n",
    "### prediction on total dataset trained using MLP and combination of all the three features \n",
    "classification(confusion_name='confusion_binary_MLP_gptpe.png',report_name='confusion_binary_MLP_gptpe.csv',pred_name_save='Counter_NonCounter/Classifier/all_preds_binary.pkl')\n",
    "\n",
    "### prediction on total dataset trained using LSTM and random embeddings \n",
    "classification(confusion_name='confusion_lstm_random_embed.png',report_name='confusion_lstm_random_embed.csv',pred_name_save='Counter_NonCounter/Classifier/all_preds_lstm.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:punyajoy-nogpu]",
   "language": "python",
   "name": "conda-env-punyajoy-nogpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
