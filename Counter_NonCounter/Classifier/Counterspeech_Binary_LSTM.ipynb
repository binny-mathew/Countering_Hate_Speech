{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task -1 : Classifying Text into Counter and Non Counter Class using LSTM\n",
    "### This notebook is used to measure performance of LSTM using random embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Input, LSTM ,GRU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Input,merge, Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import Bidirectional\n",
    "from keras import optimizers\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pdb\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.ensemble  import GradientBoostingClassifier, RandomForestClassifier\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import np_utils\n",
    "import codecs\n",
    "import operator\n",
    "import gensim, sklearn\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "#from batch_gen import batch_gen\n",
    "import sys\n",
    "from sklearn.utils import class_weight\n",
    "import wordninja\n",
    "from nltk import tokenize as tokenize_nltk\n",
    "#from my_tokenizer import glove_tokenize\n",
    "import math\n",
    "import re\n",
    "### Preparing the text data\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "# vocab generation\n",
    "vocab, reverse_vocab = {}, {}\n",
    "freq = defaultdict(int)\n",
    "tweets = {}\n",
    "from sklearn.metrics import confusion_matrix,make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function will return the plot of the confusion matrix \n",
    "1. input: confusion matrix and target names(class_name)\n",
    "2. output: plot of confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file used to write preserve the results of the classfier\n",
    "# confusion matrix and precision recall fscore matrix\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    \n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function is generating the classification report\n",
    "1. input: ground_truth and predicted outputs\n",
    "2. output: dataframe containing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##saving the classification report\n",
    "def pandas_classification_report(y_true, y_pred):\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='macro'))\n",
    "    avg.append(accuracy_score(y_true, y_pred, normalize=True))\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support','accuracy']\n",
    "    list_all=list(metrics_summary)\n",
    "    list_all.append(cm.diagonal())\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list_all,\n",
    "        index=metrics_sum_index)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    avg[-2] = total\n",
    "\n",
    "    class_report_df['avg / total'] = avg\n",
    "\n",
    "    return class_report_df.T,avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "#GLOVE_MODEL_FILE = \"../../../LEAM-master/glove.twitter.27B/glove.twitter.27B.200d.txt\"\n",
    "GLOVE_MODEL_FILE=\"../../../glove.840B.300d.txt\"\n",
    "\n",
    "print(os.path.isfile(GLOVE_MODEL_FILE))\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "\n",
    "glove_file = GLOVE_MODEL_FILE\n",
    "tmp_file = get_tmpfile(\"test_word2vec_300.txt\")\n",
    "\n",
    "# call glove2word2vec script\n",
    "# default way (through CLI): python -m gensim.scripts.glove2word2vec --input <glove_file> --output <w2v_file>\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###for fast text word embeddings\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('../../../wiki.en.vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2519370"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batches are geenrated for the min batch training environment \n",
    "def batch_gen(X, batch_size):\n",
    "    n_batches = X.shape[0]/float(batch_size)\n",
    "    n_batches = int(math.ceil(n_batches))\n",
    "    end = int(X.shape[0]/float(batch_size)) * batch_size\n",
    "    n = 0\n",
    "    for i in range(0,n_batches):\n",
    "        if i < n_batches - 1: \n",
    "            batch = X[i*batch_size:(i+1) * batch_size, :]\n",
    "            yield batch\n",
    "        \n",
    "        else:\n",
    "            batch = X[end: , :]\n",
    "            n += X[end:, :].shape[0]\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....start....cleaning\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from commen_preprocess import *\n",
    "\n",
    "### this is tokenization for word embeddings \n",
    "def glove_tokenize(text):\n",
    "    #text = tokenizer(text)\n",
    "    text=clean(text, remove_stopwords=False, remove_punctuations=False)\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in STOPWORDS]\n",
    "    #words =[ps.stem(word) for word in words]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### embeddings if present for the word ..return the embeddings else return a vector of embeddings vector 0  \n",
    "def get_embedding(word):\n",
    "    #return\n",
    "    try:\n",
    "        return word2vec_model[word]\n",
    "    except Exception as e:\n",
    "        print('Encoding not found: %s' %(word))\n",
    "        return np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "#form the embeddings matrix where ith row is the embedding vector of the ith word in the vocab    \n",
    "def get_embedding_weights():\n",
    "    embedding = np.zeros((len(vocab) + 1, EMBEDDING_DIM))\n",
    "    n = 0\n",
    "    for k, v in vocab.items():\n",
    "        try:\n",
    "            embedding[v] = word2vec_model[k]\n",
    "        except:\n",
    "            n += 1\n",
    "            pass\n",
    "    print(\"%d embedding missed\"%n)\n",
    "    return embedding\n",
    "\n",
    "#### comments and corresponding labels collection from the defined pd_train(the dataframe) \n",
    "def get_data():\n",
    "    comments=pd_train['text'].values\n",
    "    labels=pd_train['class'].values\n",
    "    list_comment=[]\n",
    "    for comment,label in zip(comments,labels):\n",
    "        temp={}\n",
    "        temp['text']=comment\n",
    "        temp['label']=label\n",
    "        list_comment.append(temp)\n",
    "    return list_comment\n",
    "\n",
    "## select those tweets only which have the embeddings present in the word2vec model\n",
    "#not required in case of random embeddings  \n",
    "def select_tweets():\n",
    "    # selects the tweets as in mean_glove_embedding method\n",
    "    # Processing\n",
    "    tweet_1 = get_data()\n",
    "    X, Y = [], []\n",
    "    tweet_return = []\n",
    "    for tweet in tweet_1:\n",
    "        _emb = 0\n",
    "        words = glove_tokenize(tweet['text'].lower())\n",
    "        for w in words:\n",
    "            if w in word2vec_model:  # Check if embeeding there in GLove model\n",
    "                _emb+=1\n",
    "        if _emb:   # Not a blank tweet\n",
    "            tweet_return.append(tweet)\n",
    "    print('Tweets selected:', len(tweet_return))\n",
    "    return tweet_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "import json\n",
    "####give the whole complete path of the file along with the filename \n",
    "path='../Data/Counterspeech_Dataset.json'\n",
    "with open(path) as fp:\n",
    "    train_data = json.load(fp)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from commen_preprocess import *\n",
    "## converting class label to counter or non counter depending on the value of the class 0 or 1 \n",
    "def convert_class_label(input_text):\n",
    "    if input_text:\n",
    "        return 'counter'\n",
    "    else:\n",
    "        return 'noncounter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Loading Completed...\n"
     ]
    }
   ],
   "source": [
    "###training datafrane created\n",
    "\n",
    "pd_train = pd.DataFrame(columns=['id','class','community','category','text'])\n",
    "\n",
    "###values inserted in the dataframe \n",
    "for count, each in enumerate(train_data):\n",
    "    try:\n",
    "        pd_train.loc[count]  = [each['id'], convert_class_label(each['CounterSpeech']), each['Community'],each['Category'],each['commentText']]\n",
    "    except:\n",
    "        pass\n",
    "print('Training Data Loading Completed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##generating the voacbs from the text\n",
    "def gen_vocab(tweets_1):\n",
    "    # Processing\n",
    "    vocab_index = 1\n",
    "    for tweet in tweets_1:\n",
    "        text = glove_tokenize(tweet['text'].lower())\n",
    "        #text = ''.join([c for c in text if c not in punctuation])\n",
    "        #words = text.split()\n",
    "        #words = [word for word in words if word not in STOPWORDS]\n",
    "        words=text\n",
    "        for word in words:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = vocab_index\n",
    "                reverse_vocab[vocab_index] = word       # generate reverse vocab as well\n",
    "                vocab_index += 1\n",
    "            freq[word] += 1\n",
    "    vocab['UNK'] = len(vocab) + 1\n",
    "    reverse_vocab[len(vocab)] = 'UNK'\n",
    "\n",
    "\n",
    "###filtering out vocab by taking the top k vocab words \n",
    "def filter_vocab(k):\n",
    "    global freq, vocab\n",
    "    pdb.set_trace()\n",
    "    freq_sorted = sorted(list(freq.items()), key=operator.itemgetter(1))\n",
    "    tokens = freq_sorted[:k]\n",
    "    vocab = dict(list(zip(tokens, list(range(1, len(tokens) + 1)))))\n",
    "    vocab['UNK'] = len(vocab) + 1\n",
    "\n",
    "### generate a X and y for training \n",
    "def gen_sequence(tweets_new):\n",
    "    y_map = {\n",
    "            'counter': 0,\n",
    "            'noncounter': 1\n",
    "             }\n",
    "\n",
    "    X, y = [], []\n",
    "    for tweet in tweets_new:\n",
    "        text = glove_tokenize(tweet['text'].lower())\n",
    "        #text = ''.join([c for c in text if c not in punctuation])\n",
    "        #words = text.split()\n",
    "        #words = [word for word in words if word not in STOPWORDS]\n",
    "        words=text\n",
    "        seq, _emb = [], []\n",
    "        for word in words:\n",
    "            seq.append(vocab.get(word, vocab['UNK']))\n",
    "        X.append(seq)\n",
    "        y.append(y_map[tweet['label']])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "###if weights are random then shuffle the weights \n",
    "\n",
    "def shuffle_weights(model):\n",
    "    weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    model.set_weights(weights)\n",
    "\n",
    "    \n",
    "###MAIN MODEL OF LSTM are defined here    \n",
    "def lstm_model(sequence_length, embedding_dim,dropout1):\n",
    "    model_variation = 'GRU'\n",
    "    print(('Model variation is %s' % model_variation))\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(vocab)+1, embedding_dim, input_length=sequence_length, trainable=LEARN_EMBEDDINGS))\n",
    "    #model.add(Dropout(dropout1))#, input_shape=(sequence_length, embedding_dim)))\n",
    "    model.add(Bidirectional(GRU(HIDDEN)))\n",
    "    #model.add(GRU(50))\n",
    "    model.add(Dropout(dropout1))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    #sgd = optimizers.SGD(lr=LR, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    adam=optimizers.Adam(lr=LR, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "from keras import backend as K\n",
    "\n",
    "###training the lstm ###  \n",
    "\n",
    "def train_LSTM(X_temp,sentence_len,model, inp_dim, weights, epochs=10, batch_size=512,INITIALIZE_WEIGHTS_WITH = \"random\"):\n",
    "        if INITIALIZE_WEIGHTS_WITH == \"glove\":\n",
    "            model.layers[0].set_weights([weights])\n",
    "        elif INITIALIZE_WEIGHTS_WITH == \"random\":\n",
    "            shuffle_weights(model)\n",
    "        else:\n",
    "            print(\"ERROR!\")\n",
    "            return\n",
    "        for epoch in range(epochs):\n",
    "            loss_all=0.0\n",
    "            acc_all=0.0\n",
    "            count=0.0\n",
    "            for X_batch in batch_gen(X_temp, batch_size):\n",
    "                x = X_batch[:, :sentence_len]\n",
    "                y_temp = X_batch[:, sentence_len]\n",
    "\n",
    "                class_weights = None\n",
    "                if SCALE_LOSS_FUN:\n",
    "                    #class_weights = {}\n",
    "                    #class_weights[0] = np.where(y_temp == 0)[0].shape[0]/float(len(y_temp))\n",
    "                    #class_weights[1] = np.where(y_temp == 1)[0].shape[0]/float(len(y_temp))\n",
    "                    #class_weights[2] = np.where(y_temp == 2)[0].shape[0]/float(len(y_temp))\n",
    "                    class_weights = class_weight.compute_class_weight('balanced',np.unique(y_temp),y_temp)\n",
    "                    #print(class_weights)\n",
    "                try:\n",
    "                    y_temp = np_utils.to_categorical(y_temp, num_classes=2)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(y_temp)\n",
    "                #print(x.shape, y_temp.shape,class_weights)\n",
    "                loss, acc = model.train_on_batch(x, y_temp, class_weight=class_weights)\n",
    "                loss_all+=loss\n",
    "                acc_all+=acc\n",
    "                count+=1.0\n",
    "            print('Avg loss:',loss_all/count, 'Avg accuracy:',acc_all/count, 'Epoch:',epoch)\n",
    "                \n",
    "        return model\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_results = pd.DataFrame(columns=['model+features','kth_fold','precision_train','recall_train','fscore_train','accuracy_train','precision_test','recall_test','fscore_test','accuracy_test'])\n",
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####for fine tuning...... fixing the parameters ####### \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "TOKENIZER = 'glove_tokenize'\n",
    "INITIALIZE_WEIGHTS_WITH = 'random'    \n",
    "LEARN_EMBEDDINGS = False\n",
    "BATCH_SIZE = 512\n",
    "HIDDEN=10\n",
    "LR=0.1\n",
    "FIXED_EPOCH = 20\n",
    "\n",
    "np.random.seed(10)\n",
    "print('embedding: %s' %(GLOVE_MODEL_FILE))\n",
    "print('Embedding Dimension: %d' %(EMBEDDING_DIM))\n",
    "print('Allowing embedding learning: %s' %(str(LEARN_EMBEDDINGS)))\n",
    "\n",
    "#### loading weights and data\n",
    "SCALE_LOSS_FUN=True\n",
    "tweets= select_tweets()\n",
    "\n",
    "#gen_vocab(tweets+sample_list)\n",
    "gen_vocab(tweets)\n",
    "#filter_vocab(20000)\n",
    "X, y = gen_sequence(tweets)\n",
    "MAX_SEQUENCE_LENGTH = max([len(x) for x in X])\n",
    "print(\"max seq length is %d\"%(MAX_SEQUENCE_LENGTH))\n",
    "data = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y = np.array(y)\n",
    "#data, y = sklearn.utils.shuffle(data, y)\n",
    "W = get_embedding_weights()\n",
    "X=data\n",
    "sentence_len = X.shape[1]\n",
    "\n",
    "with open('vocab_lstm.pickle', 'wb') as handle:\n",
    "     pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "param_sets = list(ParameterGrid(grid_param))\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "fold_count = 0\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    print('cv_fold',fold_count)\n",
    "    fold_count += 1\n",
    "    X_train_temp, X_test = X[train_index], X[test_index]\n",
    "    y_train_temp, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    y_train_temp = y_train_temp.reshape((len(y_train_temp), 1))\n",
    "\n",
    "    X_temp = np.hstack((X_train_temp, y_train_temp))\n",
    "\n",
    "    model = lstm_model(data.shape[1], EMBEDDING_DIM,0.5)\n",
    "    model=train_LSTM(X_temp ,sentence_len, model, EMBEDDING_DIM, W,epochs=FIXED_EPOCH, batch_size=512,INITIALIZE_WEIGHTS_WITH =\"glove\")\n",
    "\n",
    "    y_preds = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train_temp)\n",
    "    y_preds=[np.argmax(y_pred_1) for y_pred_1 in y_preds]\n",
    "    y_pred_train=[np.argmax(y_pred_1) for y_pred_1 in y_pred_train]\n",
    "    print('accuracy_train:',accuracy_score(y_train_temp, y_pred_train),'accuracy_test:',accuracy_score(y_test, y_preds))\n",
    "    df_result_tr,avg_tr=pandas_classification_report(y_train_temp,y_pred_train)\n",
    "    df_result_te,avg_te=pandas_classification_report(y_test,y_preds) \n",
    "    pd_results.loc[count]  = ['GRU_finetuning_glove',fold_count,avg_tr[0],avg_tr[1],avg_tr[2],avg_tr[4],avg_te[0],avg_te[1],avg_te[2],avg_te[4]]\n",
    "    K.clear_session()\n",
    "    count=count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_results.to_csv(\"lstm_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
